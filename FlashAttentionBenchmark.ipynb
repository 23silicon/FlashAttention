{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOv6T9QvFqn8p6sRpDwoSK0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/23silicon/FlashAttention/blob/main/FlashAttentionBenchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ninja"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwmSr_WDizTs",
        "outputId": "1d440507-06bd-46e8-8b81-8a64f16b0f9d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (1.13.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZmMdc8xzSrnR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import sys\n",
        "from torch.utils.cpp_extension import load_inline\n",
        "!rm -rf /root/.cache/torch_extensions/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8x2FwPUoXX",
        "outputId": "ef177056-2fee-4f51-fe61-26bb575b57dd"
      },
      "source": [
        "if not torch.cuda.is_available():\n",
        "  print(\"Cuda not available\")\n",
        "  exit()\n",
        "\n",
        "cuda_flash_attention = \"\"\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <cmath>\n",
        "\n",
        "#define Br 32 //canonical Q tile height name in FlashAttention paper\n",
        "#define Bc 32 //canonical K/V tiles height\n",
        "\n",
        "__global__ void flash_attention(const float* Q, const float* K, const float* V, float* output,\n",
        "                                int M, int N, int d, int Tr, int Tc, float scale) {\n",
        "    int id = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    extern __shared__ float sram[];\n",
        "    //pointers to the beginning of each tile's allocated region in shared memory\n",
        "    float* Qtile = sram; //rows 0-Br are for Q tile, height Br\n",
        "    float* Ktile = &sram[Br * d]; //rows Br-(Br+Bc) are for K tile, height Bc\n",
        "    float* Vtile = &sram[(Br + Bc) * d]; //rows (Br+Bc) to end are for Vtile, height Bc\n",
        "\n",
        "\n",
        "    //loop 1: load tiles of Q into sram.\n",
        "    // ***Each thread is responsible for loading 1 full row of Q into its respective tile\n",
        "    if (d % 4 == 0) {\n",
        "        for (int i = 0; i < (d >> 2); i++) {\n",
        "            /*\n",
        "            GPU doesn't retrieve a single float per query, instead it retrieves up to 32 bytes.\n",
        "            Therefore, it's very inefficient to load 4 bytes at a time into sram.\n",
        "            */\n",
        "            float4* Q4 = (float4*)Q;\n",
        "            float4* Q4tile = (float4*)Qtile;\n",
        "            Q4tile[threadIdx.x * (d >> 2) + i] = Q4[id * (d >> 2) + i];\n",
        "        }\n",
        "    } else {\n",
        "        for (int i = 0; i < d; i++) {\n",
        "            Qtile[threadIdx.x * d + i] = Q[id * d + i];\n",
        "        }\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "\n",
        "    //callocs to 0, fixed size typical max for d_model is 128 and explodes to registers\n",
        "    float acc[128] = {0.0f}; //O matrix accumulator\n",
        "    float l = 0.0f; //running denominator sum for softmax\n",
        "    float m = -1e30f; //starts with a very low number, basically -infinity\n",
        "\n",
        "    //ENTERING MAIN LOOP: here we stream tiles of K and V\n",
        "    for (int i = 0; i < Tc; i++) {\n",
        "        //load K/V\n",
        "        if (d % 4 == 0) {\n",
        "            float4* K4 = (float4*)K;\n",
        "            float4* V4 = (float4*)V;\n",
        "            float4* K4tile = (float4*)Ktile;\n",
        "            float4* V4tile = (float4*)Vtile;\n",
        "            for (int j = 0; j < d >> 2; j++) {\n",
        "                int tilerow_KV = Bc * i + threadIdx.x;\n",
        "                if (tilerow_KV < N) {\n",
        "                    K4tile[threadIdx.x * (d >> 2) + j] = K4[tilerow_KV * (d >> 2) + j];\n",
        "                    V4tile[threadIdx.x * (d >> 2) + j] = V4[tilerow_KV * (d >> 2) + j];\n",
        "                } else {\n",
        "                    K4tile[threadIdx.x * (d >> 2) + j] = make_float4(0.0f, 0.0f, 0.0f, 0.0f);\n",
        "                    V4tile[threadIdx.x * (d >> 2) + j] = make_float4(0.0f, 0.0f, 0.0f, 0.0f);\n",
        "                }\n",
        "            }\n",
        "        } else {\n",
        "            for (int j = 0; j < d; j++) {\n",
        "                int tilerow_KV = Bc * i + threadIdx.x;\n",
        "                Ktile[threadIdx.x * d + j] = (tilerow_KV < N) ? K[tilerow_KV * d + j] : 0.0f;\n",
        "                //V tile loaded by row as well to compute streamed dot product at the end\n",
        "                Vtile[threadIdx.x * d + j] = (tilerow_KV < N) ? V[tilerow_KV * d + j] : 0.0f;\n",
        "            }\n",
        "        }\n",
        "        __syncthreads();\n",
        "\n",
        "        //Next step: compute dot product of this thread's corresponding Q row and every Ktile row (dim: Bc x d)\n",
        "        float attention_scores[Bc];\n",
        "        float blockmax = -1e30f;\n",
        "        #pragma unroll //compiler hint to unroll cus Bc is known at compile time\n",
        "        for (int row = 0; row < Bc; row++) {\n",
        "            float global_K_row = i * Bc + row;\n",
        "            if (global_K_row < N) {\n",
        "                float sum = 0.0f;\n",
        "                for (int col = 0; col < d; col++) {\n",
        "                    sum += Qtile[threadIdx.x * d + col] * Ktile[row * d + col];\n",
        "                }\n",
        "                sum *= scale;\n",
        "                blockmax = max(blockmax, sum);\n",
        "                attention_scores[row] = sum;\n",
        "            } else {\n",
        "                //check for padded rows to set score to -inf instead of 0 because e^0 is 1, not 0.\n",
        "                attention_scores[row] = -1e30f;\n",
        "            }\n",
        "        }\n",
        "\n",
        "        //Most technically beefy part of this kernel: online safe softmax\n",
        "\n",
        "        //part 1: Adjust attention scores by subtracting blockmax from each element, find running sum\n",
        "        float blocksum = 0.0f;\n",
        "        for (int j = 0; j < Bc; j++) {\n",
        "            attention_scores[j] = __expf(attention_scores[j]-blockmax);\n",
        "            blocksum += attention_scores[j];\n",
        "        }\n",
        "\n",
        "        //part 2: calculate new global max and scaling factors\n",
        "        float newmax = max(m, blockmax);\n",
        "        float scale_f1 = __expf(m-newmax);\n",
        "        float scale_fb = __expf(blockmax - newmax);\n",
        "\n",
        "        m = newmax;\n",
        "        l = (l * scale_f1) + (blocksum * scale_fb); //apply scaling factors\n",
        "\n",
        "        //part 3: adjust and update accumulator\n",
        "        for (int col = 0; col < d; col++) {\n",
        "            acc[col] *= scale_f1;\n",
        "        }\n",
        "        for (int j = 0; j < Bc; j++) {\n",
        "            float scaled_p = attention_scores[j] * scale_fb;\n",
        "            for (int col = 0; col < d; col++) {\n",
        "                acc[col] += scaled_p * Vtile[j * d + col];\n",
        "            }\n",
        "        }\n",
        "\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    //divide each element by l to complete the softmax and write to output\n",
        "    if (id < M) { //final matrix is M x d, this block write the entire vector of length d to row @id\n",
        "        float divL = 1.0f/l; //avoid repeated division\n",
        "        if (d % 4 == 0) {\n",
        "             float4* O4 = (float4*)output;\n",
        "             for (int col = 0; col < d / 4; col++) {\n",
        "                float4 res;\n",
        "                res.x = acc[col * 4 + 0] * divL;\n",
        "                res.y = acc[col * 4 + 1] * divL;\n",
        "                res.z = acc[col * 4 + 2] * divL;\n",
        "                res.w = acc[col * 4 + 3] * divL;\n",
        "                O4[id * (d / 4) + col] = res;\n",
        "             }\n",
        "        } else {\n",
        "            for (int col = 0; col < d; col++) {\n",
        "                output[id * d + col] = acc[col] * divL;\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "// Q, K, V, output are device pointers\n",
        "extern \"C\" void solve_flash(const float* Q, const float* K, const float* V, float* output, int M, int N,\n",
        "                      int d) {\n",
        "    //Q is Mxd, K is Nxd, V is Nxd\n",
        "    //QK^T is MxN\n",
        "    //output is MxN\n",
        "    int Tr = (M + Br - 1) / Br, Tc = (N + Bc - 1) / Bc; //tile counts\n",
        "    dim3 threadsPerBlock (Br);\n",
        "    dim3 blocksPerGrid (Tr);\n",
        "    int sram_size ((Br * d + Bc * d + Bc * d) * sizeof(float)); //1 tile of Q, 1 tile of K, 1 tile of V\n",
        "    float scale = 1.0f/sqrtf(d); //scaling factor multiplied by each element before softmax\n",
        "\n",
        "    flash_attention<<<blocksPerGrid, threadsPerBlock, sram_size>>>(Q, K, V, output, M, N, d, Tr, Tc, scale);\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "cuda_self_attention = \"\"\"\n",
        "#include <cuda_runtime.h>\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "#include <cfloat>\n",
        "\n",
        "const int TILE_SIZE = 16;\n",
        "\n",
        "__global__ void matrix_multiplication_kernel(const float* A, const float* B, float* C, int M, int d, int N, const float factor, const bool transposed) {\n",
        "    const int nx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    const int ny = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    if (nx < N && ny < M) {\n",
        "        int idx = ny * N + nx;\n",
        "        float sum = 0;\n",
        "        if (transposed) {\n",
        "            for (int i = 0; i < d; i++) {\n",
        "                sum += A[ny * d + i] * B[nx * d + i];\n",
        "            }\n",
        "        } else {\n",
        "            for (int i = 0; i < d; i++) {\n",
        "                sum += A[ny * d + i] * B[i * N + nx];\n",
        "            }\n",
        "        }\n",
        "        C[idx] = sum * factor;\n",
        "    }\n",
        "}\n",
        "\n",
        "__device__ float atomicMaxFloat(float* address, float val) {\n",
        "    int* address_as_int = (int*)address;\n",
        "    int old = *address_as_int, assumed;\n",
        "\n",
        "    do {\n",
        "        assumed = old;\n",
        "        old = atomicCAS(address_as_int, assumed, __float_as_int(fmaxf(val, __int_as_float(assumed))));\n",
        "    } while (assumed != old);\n",
        "\n",
        "    return __int_as_float(old);\n",
        "}\n",
        "\n",
        "__global__ void row_max_kernel(const float* input, float* row_max, const int M, const int N) {\n",
        "    const int nx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    const int ny = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    const int idx = ny * N + nx;\n",
        "    const int tid = threadIdx.x;\n",
        "    const int bid = threadIdx.y;\n",
        "    __shared__ float sd[TILE_SIZE][TILE_SIZE];\n",
        "    sd[bid][tid] = (nx < N && ny < M) ? input[idx] : FLT_MIN;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int offset = blockDim.x >> 1; offset > 0; offset >>= 1) {\n",
        "        if (tid < offset && sd[bid][tid] < sd[bid][tid + offset]) {\n",
        "            sd[bid][tid] = sd[bid][tid + offset];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        atomicMaxFloat(&row_max[ny], sd[bid][0]);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void row_sum_kernel(const float* input, float* row_max, float* row_sum, const int M, const int N) {\n",
        "    const int nx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    const int ny = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    const int idx = ny * N + nx;\n",
        "    const int tid = threadIdx.x;\n",
        "    const int bid = threadIdx.y;\n",
        "    __shared__ float sd[TILE_SIZE][TILE_SIZE];\n",
        "    sd[bid][tid] = (nx < N && ny < M) ? expf(input[idx] - row_max[ny]) : 0.0;\n",
        "    __syncthreads();\n",
        "\n",
        "    for (int offset = blockDim.x >> 1; offset > 0; offset >>= 1) {\n",
        "        if (tid < offset) {\n",
        "            sd[bid][tid] += sd[bid][tid + offset];\n",
        "        }\n",
        "        __syncthreads();\n",
        "    }\n",
        "\n",
        "    if (tid == 0) {\n",
        "        atomicAdd(&row_sum[ny], sd[bid][0]);\n",
        "    }\n",
        "}\n",
        "\n",
        "__global__ void softmax_kernel(const float* input, float* output, float* row_max, float* row_sum, const int M, const int N) {\n",
        "    const int nx = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    const int ny = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "    if (nx < N && ny < M) {\n",
        "        const int idx = ny * N + nx;\n",
        "        output[idx] = expf(input[idx] - row_max[ny]) / row_sum[ny];\n",
        "    }\n",
        "}\n",
        "\n",
        "void softmax(const float* input, float* output, int M, int N) {\n",
        "    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n",
        "    dim3 blocksPerGrid((N + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                       (M + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "\n",
        "    float *row_max;\n",
        "    cudaMalloc((void **)&row_max, sizeof(float) * M);\n",
        "    row_max_kernel<<<blocksPerGrid, threadsPerBlock>>>(input, row_max, M, N);\n",
        "\n",
        "    float *row_sum;\n",
        "    cudaMalloc((void **)&row_sum, sizeof(float) * M);\n",
        "    cudaMemset(row_sum, 0, sizeof(float) * M);\n",
        "    row_sum_kernel<<<blocksPerGrid, threadsPerBlock>>>(input, row_max, row_sum, M, N);\n",
        "\n",
        "\n",
        "    softmax_kernel<<<blocksPerGrid, threadsPerBlock>>>(input, output, row_max, row_sum, M, N);\n",
        "    cudaFree(row_max);\n",
        "    cudaFree(row_sum);\n",
        "}\n",
        "\n",
        "// Q, K, V, output are device pointers\n",
        "extern \"C\" void solve_naive(const float* Q, const float* K, const float* V, float* output, int M, int N, int d) {\n",
        "    const int SIZE = M * N;\n",
        "    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n",
        "\n",
        "    float* qk;\n",
        "    cudaMalloc((void **)&qk, sizeof(float) * SIZE);\n",
        "    dim3 blocksPerGrid((N + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                       (M + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "    const float factor = 1 / sqrt((float)d);\n",
        "    matrix_multiplication_kernel<<<blocksPerGrid, threadsPerBlock>>>(Q, K, qk, M, d, N, factor, true);\n",
        "\n",
        "    float* softMaxQK;\n",
        "    cudaMalloc((void **)&softMaxQK, sizeof(float) * SIZE);\n",
        "    softmax(qk, softMaxQK, M, N);\n",
        "    cudaFree(qk);\n",
        "\n",
        "    dim3 blocksPerGrid2((d + threadsPerBlock.x - 1) / threadsPerBlock.x,\n",
        "                       (M + threadsPerBlock.y - 1) / threadsPerBlock.y);\n",
        "    matrix_multiplication_kernel<<<blocksPerGrid2, threadsPerBlock>>>(softMaxQK, V, output, M, N, d, 1.0, false);\n",
        "    cudaFree(softMaxQK);\n",
        "}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "cpp_source = \"\"\"\n",
        "extern \"C\" void solve_flash(const float* Q, const float* K, const float* V, float* output, int M, int N, int d);\n",
        "extern \"C\" void solve_naive(const float* Q, const float* K, const float* V, float* output, int M, int N, int d);\n",
        "\n",
        "torch::Tensor run_flash(torch::Tensor Q, torch::Tensor K, torch::Tensor V) {\n",
        "    auto M = Q.size(0);\n",
        "    auto N = K.size(0);\n",
        "    auto d = Q.size(1);\n",
        "    auto output = torch::empty_like(Q);\n",
        "    solve_flash(Q.data_ptr<float>(), K.data_ptr<float>(), V.data_ptr<float>(),\n",
        "                output.data_ptr<float>(), M, N, d);\n",
        "    return output;\n",
        "}\n",
        "\n",
        "torch::Tensor run_naive(torch::Tensor Q, torch::Tensor K, torch::Tensor V) {\n",
        "    auto M = Q.size(0);\n",
        "    auto N = K.size(0);\n",
        "    auto d = Q.size(1);\n",
        "    auto output = torch::empty_like(Q);\n",
        "    solve_naive(Q.data_ptr<float>(), K.data_ptr<float>(), V.data_ptr<float>(),\n",
        "                output.data_ptr<float>(), M, N, d);\n",
        "    return output;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "print(\"Compiling Kernels...\")\n",
        "kernels = load_inline(\n",
        "    name='comparison_kernels',\n",
        "    cpp_sources=cpp_source,\n",
        "    cuda_sources=cuda_flash_attention + \"\\n\" + cuda_self_attention,\n",
        "    functions=['run_flash', 'run_naive'],\n",
        "    extra_cuda_cflags=['-O3', '--use_fast_math'],\n",
        "    verbose=True\n",
        ")\n",
        "print(\"Compilation Finished!\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compiling Kernels...\n",
            "Compilation Finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def profile(func, args, name):\n",
        "    # Warmup\n",
        "    for _ in range(3): func(*args)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    start.record()\n",
        "    for _ in range(10):\n",
        "        func(*args)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    return start.elapsed_time(end) / 10\n",
        "\n",
        "def pytorch_sdpa(Q, K, V):\n",
        "    # SDPA expects 4D input: (Batch, Heads, Seq, Dim)\n",
        "    # We fake Batch=1, Heads=1\n",
        "    return torch.nn.functional.scaled_dot_product_attention(\n",
        "        Q.view(1, 1, Q.size(0), Q.size(1)),\n",
        "        K.view(1, 1, K.size(0), K.size(1)),\n",
        "        V.view(1, 1, V.size(0), V.size(1))\n",
        "    ).view(Q.size(0), Q.size(1))\n",
        "\n",
        "def run_benchmark():\n",
        "    d = 128\n",
        "    print(f\"\\n{'N':<8} | {'Naive (ms)':<15} | {'Flash (ms)':<15} | {'PyTorch (ms)':<15} | {'Speedup'}\")\n",
        "    print(\"-\" * 85)\n",
        "\n",
        "    for N in [2<<14, 2<<15, 2<<16]: # 32k, 64k, 128k\n",
        "\n",
        "        Q = torch.randn(N, d, device='cuda')\n",
        "        K = torch.randn(N, d, device='cuda')\n",
        "        V = torch.randn(N, d, device='cuda')\n",
        "\n",
        "        # 1. Naive Benchmark (Isolated)\n",
        "        t_naive = \"OOM\"\n",
        "        try:\n",
        "            # Only run Naive if N is reasonable to avoid hanging the colab\n",
        "            if N <= 32768:\n",
        "                t_naive = profile(kernels.run_naive, (Q,K,V), \"Naive\")\n",
        "                t_naive_val = t_naive\n",
        "            else:\n",
        "                t_naive = \"OOM (Skip)\"\n",
        "                t_naive_val = float('inf')\n",
        "        except Exception as e:\n",
        "            t_naive = \"OOM\"\n",
        "            t_naive_val = float('inf')\n",
        "\n",
        "        # 2. Flash Benchmark (Always Run)\n",
        "        try:\n",
        "            t_flash = profile(kernels.run_flash, (Q,K,V), \"Flash\")\n",
        "        except Exception as e:\n",
        "            t_flash = \"Error\"\n",
        "            print(e)\n",
        "\n",
        "        # 3. SDPA Benchmark (Always Run)\n",
        "        try:\n",
        "            t_sdpa = profile(pytorch_sdpa, (Q,K,V), \"SDPA\")\n",
        "        except:\n",
        "            t_sdpa = \"Error\"\n",
        "\n",
        "        # Format speedup\n",
        "        if isinstance(t_naive, str):\n",
        "            speedup = \"Inf\"\n",
        "        else:\n",
        "            speedup = f\"{t_naive/t_flash:.2f}x\"\n",
        "\n",
        "        tn_str = f\"{t_naive:.4f}\" if not isinstance(t_naive, str) else t_naive\n",
        "        tf_str = f\"{t_flash:.4f}\" if not isinstance(t_flash, str) else t_flash\n",
        "        ts_str = f\"{t_sdpa:.4f}\" if not isinstance(t_sdpa, str) else t_sdpa\n",
        "\n",
        "        print(f\"{N:<8} | {tn_str:<15} | {tf_str:<15} | {ts_str:<15} | {speedup}\")\n",
        "\n",
        "run_benchmark()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdVnuP-MitFE",
        "outputId": "70028c6d-77c6-40e6-c358-b8a5b951080a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "N        | Naive (ms)      | Flash (ms)      | PyTorch (ms)    | Speedup\n",
            "-------------------------------------------------------------------------------------\n",
            "32768    | 3743.8746       | 6541.5773       | 198.0648        | 0.57x\n",
            "65536    | OOM (Skip)      | 26291.5719      | 981.7722        | Inf\n",
            "131072   | OOM (Skip)      | 104102.6187     | 4297.5219       | Inf\n"
          ]
        }
      ]
    }
  ]
}